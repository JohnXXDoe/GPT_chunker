{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import flair\n",
    "from flair.splitter import SegtokSentenceSplitter\n",
    "from flair.splitter import SegtokTokenizer\n",
    "from io import BytesIO\n",
    "import textract\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "import platform\n",
    "import re\n",
    "import nltk\n",
    "import openai\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def read_all_pdf():\n",
    "    escape_seq = '\\n'\n",
    "    excel, ppt, word = [], [], []\n",
    "    err_files = []\n",
    "    err_log = []\n",
    "    file_names = []\n",
    "    page_wise_data = []\n",
    "    folder_path = os.path.join('Documents', 'AIA_Basefiles') #input(\"Enter folder location: \")\n",
    "\n",
    "    file_n, file_info = [],[]\n",
    "\n",
    "    str1=[]\n",
    "\n",
    "    # Walk through all files and subdirectories in the folder\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        try:\n",
    "            for index, file in enumerate(files):\n",
    "                if file.endswith(\".pdf\"):\n",
    "                    \n",
    "                    pdf_location = os.path.abspath(os.path.join(root, file))\n",
    "                    print(pdf_location)\n",
    "                    page_wise_data = pdfdata(pdf_location)\n",
    "                    \n",
    "                    file_n.append(file)\n",
    "                    file_info.append(\"\\n\".join(page_wise_data))\n",
    "\n",
    "                    #dataframe = df.assign(Contents = pa\\ge_wise_data)\n",
    "                    # df['Contents'] = page_wise_data   #Extract text from PDF\n",
    "                    file_names.append(file)\n",
    "                    print(f'[File Number]:: {index}\\n [File Name]:: {file_n[-1]}\\n[File count:]{len(file_n)}')\n",
    "                    # count += 1\n",
    "                elif file.endswith(\".xlsx\") or file.endswith(\".csv\") or file.endswith(\".xlsb\"):\n",
    "                    excel.append(file)\n",
    "                elif file.endswith(\".ppt\"):\n",
    "                    ppt.append(file)\n",
    "                elif file.endswith(\".docx\") or file.endswith(\".doc\"):\n",
    "                    word.append(file)\n",
    "                    print(f'[File Number]:: {index}\\n [File Name]:: {file_n[-1]}\\n[File count:]{len(file_n)}')\n",
    "                    doc_location = os.path.abspath(os.path.join(root, file))\n",
    "                    text = textract.process(doc_location)\n",
    "                    file_names.append(file)\n",
    "                    file_info.append(text)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            \n",
    "            print(f'///////////////////////////\\n\\n |Random ERORR| {e}, continuing PDF extraction\\n\\n/////////////////////////////////////')\n",
    "            err_files.append(pdf_location)\n",
    "            err_log.append(e)\n",
    "            continue \n",
    "    print(f'Lengths: {len(file_info)} {len(file_n)}')\n",
    "    return(file_n, file_info)            \n",
    "\n",
    "def pdfdata(loc):\n",
    "    escape_seq = '\\n'\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = BytesIO()\n",
    "    codec = 'utf-8'\n",
    "    data = []\n",
    "    laparams = LAParams(char_margin=30, line_margin=2, boxes_flow=1)\n",
    "    device = TextConverter(rsrcmgr, retstr, codec='utf-8', laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    caching = True\n",
    "    fp = open(loc, 'rb')\n",
    "    pagenumber = 0\n",
    "    pagenos=set()\n",
    "    for pagenumber, page in enumerate(PDFPage.get_pages(fp,pagenos, maxpages=0,caching=True, check_extractable=True)):\n",
    "        #Process pages one by one into interpreter\n",
    "        if pagenumber is not None:\n",
    "            temp_str = ''\n",
    "            interpreter.process_page(page)\n",
    "            if len(retstr.getvalue()) < 25:\n",
    "               continue\n",
    "            else:\n",
    "                temp_str = retstr.getvalue().decode('ascii', 'ignore')  # add extracted text from bytesIO to data variable\n",
    "                temp_str = temp_str.replace('\\x0c \\xe2 \\x80 \\x9d', '')    # Remove useless character\n",
    "                temp_str = re.sub('\\n+', '. ', temp_str)\n",
    "                data.append(temp_str)\n",
    "        retstr.truncate(0)\n",
    "        retstr.seek(0)\n",
    "                \n",
    "    # print(f' >>>>>>NORMALSTART<<<<<< {escape_seq.join(data)} >>>>>>>>END<<<<<<<')\n",
    "    #print(f' >>>>>>START<<<<<< {sent_text} >>>>>>>>END<<<<<<<')\n",
    "    return(data)\n",
    "    \n",
    "def create_dict(folder):\n",
    "    file_n, file_info = read_all_pdf()\n",
    "    file_path = os.path.join(folder, 'raw_files.pkl')\n",
    "    # dictt = {\"File Name\": file_n, \"File Text\": file_info}\n",
    "    # n=1\n",
    "    cleaned_text=[]  \n",
    "    matched_names = []                                       \n",
    "    for i in range(0,len(file_n)-1):\n",
    "        print(f'File under check: {file_n[i]}')\n",
    "        file_raw_text = str(file_info[i])\n",
    "        file_raw_text = file_raw_text.replace(\"\\n\",\". \")\n",
    "        cleaned_text.append(file_raw_text)\n",
    "        matched_names.append(file_n[i])\n",
    "        file_raw_text=[]\n",
    "        \n",
    "    print(f'Printing length:: {len(matched_names)},{len(cleaned_text)}')\n",
    "\n",
    "\n",
    "    final_dic ={\"title\": matched_names , \"text\": cleaned_text}\n",
    "    df = pd.DataFrame.from_dict(final_dic)\n",
    "    \n",
    "    df.to_pickle(file_path)\n",
    "    chuncked_files(df, folder)\n",
    "\n",
    "def chuncked_files(df, folder):\n",
    "    sentence_splitter = SegtokSentenceSplitter()\n",
    "    word_tokeniser = SegtokTokenizer()\n",
    "    chunk = []\n",
    "    token_counter = 0\n",
    "    dic_text = []\n",
    "    dic_title = []\n",
    "    limit = 2000\n",
    "    \n",
    "    location = os.path.join(folder, 'GPT_Chuncked.pkl')\n",
    "\n",
    "    new_df = pd.DataFrame(columns = ['title', 'text'])\n",
    "\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        text = df.loc[i, 'text']\n",
    "        title = df.loc[i, 'title']\n",
    "        # print(f'Text in process: {text}')\n",
    "        sentences = sentence_splitter.split(text)\n",
    "        n_tokens = [len(word_tokeniser.tokenize(sentence.text)) for sentence in sentences]\n",
    "        \n",
    "        for sentence, tokens in zip(sentences, n_tokens):\n",
    "            # print(f'Sentence tokens: {len(word_tokeniser.tokenize(sentence.text))} || Tokens from ZIP {tokens}')\n",
    "            if token_counter < limit and tokens > 0:\n",
    "                token_counter += tokens\n",
    "                # print(f'Adding total tokens {token_counter} Limit {limit} || Appending sentence')\n",
    "                print(sentence.text)\n",
    "                chunk.append(sentence.text)\n",
    "            else:\n",
    "                # print(f'Token exceeding {limit}, appening sentence to Chunk')\n",
    "                dic_title.append(title)\n",
    "                dic_text.append(\" \".join(chunk))\n",
    "                token_counter = 0\n",
    "                chunk = []\n",
    "                print(f'Chunck cleared: {chunk}')\n",
    "\n",
    "    dict = {'filename': dic_title, 'text': dic_text}\n",
    "    new_df = pd.DataFrame(dict)\n",
    "    new_df.to_pickle(location)\n",
    "\n",
    "def create_embeddings(folder):\n",
    "    openai.api_type = \"azure\"\n",
    "    openai.api_base = \"https://teamsgptapi.openai.azure.com/\"\n",
    "    openai.api_version = \"2023-05-15\"\n",
    "    openai.api_key =\"2ae1f9e06b9346748476dcebbdb85a18\"\n",
    "    print(\"**************************************************\")\n",
    "\n",
    "    infile = os.path.join(folder, 'GPT_Chuncked.pkl')\n",
    "    outfile = os.path.join(folder, 'chunked_embeddings.pkl')\n",
    "    \n",
    "    df_chunk = pd.read_pickle(infile)\n",
    "    print(\"###########\")\n",
    "    df_chunk['embeddings'] = df_chunk.apply(lambda a: openai.Embedding.create(input=a['text'], engine='embedding-ada')['data'][0]['embedding'], axis=1)\n",
    "\n",
    "    print(\"<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "    print('DF for embeddings: ')\n",
    "    print(df_chunk['embeddings'])\n",
    "    # df['embeddings'] = df[2][0].apply(eval)\n",
    "\n",
    "    # embed = df[\"embeddings\"].tolist()\n",
    "    # print(embed)\n",
    "    # df.to_csv('testing14.csv')\n",
    "\n",
    "    print(df_chunk.head())\n",
    "    df_chunk.to_pickle(outfile)\n",
    "\n",
    "folder = \"E:/PycharmProjects/Notebooks/data\"\n",
    "create_dict(folder)\n",
    "create_embeddings(folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
